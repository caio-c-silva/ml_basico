{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVIbhRJMeAnF"
   },
   "source": [
    "# Introdução aos Modelos de Linguagem\n",
    "\n",
    "Os **modelos de linguagem** são algoritmos que têm como objetivo processar, entender e gerar linguagem natural. Eles são treinados em grandes conjuntos de dados textuais e utilizam técnicas de aprendizado de máquina, como **redes neurais** profundas, para identificar padrões e estruturas na linguagem. Esses modelos têm diversas aplicações, como:\n",
    "\n",
    "- **Tradução automática**: Converter textos de uma língua para outra.\n",
    "- **Geração de texto**: Criar novos textos a partir de um prompt inicial.\n",
    "- **Análise de sentimentos**: Identificar emoções ou opiniões expressas em textos.\n",
    "- **Assistentes virtuais**: Como chatbots e sistemas de resposta automatizada.\n",
    "\n",
    "A partir de arquiteturas avançadas, como o **Transformer** (base do modelo GPT), os modelos de linguagem modernos conseguem produzir textos cada vez mais coerentes e contextualmente relevantes, sendo capazes de lidar com tarefas de conversação, resposta a perguntas, e até mesmo de criação literária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diKEY-yOgkUP"
   },
   "source": [
    "# LLaMA\n",
    "\n",
    "O **LLaMA** (*Large Language Model Meta AI*) é uma família de modelos de linguagem desenvolvida pela **Meta AI** com o objetivo de fornecer um modelo poderoso e eficiente para tarefas de processamento de linguagem natural (PLN). Ele foi projetado para ser altamente escalável, permitindo que modelos menores atinjam um desempenho competitivo com grandes modelos de linguagem, como o GPT-3.\n",
    "\n",
    "Algumas das características do LLaMA incluem:\n",
    "\n",
    "- **Arquitetura baseada em transformadores**: Seguindo a estrutura popularizada por modelos como o GPT, o LLaMA utiliza camadas de transformadores para modelar a linguagem.\n",
    "- **Eficiência**: O LLaMA foi otimizado para ser mais eficiente, o que significa que modelos menores podem ser utilizados para realizar tarefas complexas de PLN com menos recursos computacionais.\n",
    "- **Tamanhos de modelo**: Ele está disponível em diferentes tamanhos, variando de 7B a 65B parâmetros, tornando-o acessível para diversas aplicações, desde dispositivos com menos poder de processamento até grandes servidores.\n",
    "\n",
    "O LLaMA foi desenvolvido para ser utilizado em uma ampla variedade de tarefas, incluindo:\n",
    "\n",
    "- **Geração de texto**: Criação de respostas e textos a partir de um prompt.\n",
    "- **Tradução de linguagem natural**: Conversão de texto de uma língua para outra.\n",
    "- **Resposta a perguntas**: Fornecer respostas contextualmente relevantes para questões específicas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWnMbHC5vjRb",
    "outputId": "a2dd757a-40b7-4bdf-c887-b33585ddf049"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh # baixar api do ollama\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Iniciar a API Ollama em uma thread separada \n",
    "\n",
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "ollama_thread = threading.Thread(target=ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDH4JaAIvtuj"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!ollama pull llama3.1:8b\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tqsih5Nawjr3",
    "outputId": "5043ba18-0dce-408a-e238-9db311242d20"
   },
   "outputs": [],
   "source": [
    "!pip install -U lightrag[ollama] # instalando a biblioteca AdalFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2UChfiwxlWG"
   },
   "outputs": [],
   "source": [
    "from lightrag.core.generator import Generator\n",
    "from lightrag.core.component import Component\n",
    "from lightrag.core.model_client import ModelClient\n",
    "from lightrag.components.model_client import OllamaClient, GroqAPIClient\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDmB6y7Txssl"
   },
   "outputs": [],
   "source": [
    "qa_template = r\"\"\"<SYS>\n",
    "You are a helpful assistant.\n",
    "</SYS>\n",
    "User: {{input_str}}\n",
    "You:\"\"\"\n",
    "\n",
    "class SimpleQA(Component):\n",
    "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=qa_template,\n",
    "        )\n",
    "\n",
    "    def call(self, input: dict) -> str:\n",
    "        return self.generator.call({\"input_str\": str(input)})\n",
    "\n",
    "    async def acall(self, input: dict) -> str:\n",
    "        return await self.generator.acall({\"input_str\": str(input)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "S-lNYpyFyDQF",
    "outputId": "5d0a5e49-9894-4826-de9d-b498f35ef37f"
   },
   "outputs": [],
   "source": [
    "from lightrag.components.model_client import OllamaClient\n",
    "from IPython.display import Markdown, display\n",
    "model = {\n",
    "    \"model_client\": OllamaClient(),\n",
    "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
    "}\n",
    "qa = SimpleQA(**model)\n",
    "output=qa(\"o que é uma equação diferencial?\")\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
